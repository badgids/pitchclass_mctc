{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute Features\n",
    "  \n",
    "This notebook shows how to compute an initial feature representation (Harmonic Constant-Q Transform or HCQT) from an audio file and how to convert pitch-class annotations from a csv list of note events to an output representation for training a deep pitch-class extractor.\n",
    "\n",
    "&copy; Christof Weiss and Geoffroy Peeters, Télécom Paris 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "basepath = os.path.abspath(os.path.dirname(os.path.dirname('.')))\n",
    "sys.path.append(basepath)\n",
    "import numpy as np, os, scipy, scipy.spatial, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "from numba import jit\n",
    "import librosa\n",
    "import libfmp.b, libfmp.c3, libfmp.c5\n",
    "import pandas as pd, pickle, re\n",
    "from numba import jit\n",
    "from libdl.data_preprocessing import compute_hopsize_cqt, compute_hcqt, compute_efficient_hcqt, compute_annotation_array_nooverlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load audio\n",
    "\n",
    "Load an audio file from the [Schubert Winterreise Dataset](https://doi.org/10.5281/zenodo.5139893). This serves to illustrate the extracted representations, corresponding to the following score example (Song 23 from _Winterreise_, please note that the version sung by R. Scarlata (SC06) is two semitones lower):\n",
    "\n",
    "<img src=\"data/Schubert_Winterreise/score_example.png\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 22050\n",
    "audio_folder = os.path.join(basepath, 'data', 'Schubert_Winterreise', 'audio_wav')\n",
    "\n",
    "# fn_audio = 'Schubert_D911-02_HU33.wav'\n",
    "# fn_audio = 'Schubert_D911-02_SC06.wav'\n",
    "# fn_audio = 'Schubert_D911-23_HU33.wav'\n",
    "fn_audio = 'Schubert_D911-23_SC06.wav'\n",
    "\n",
    "path_audio = os.path.join(audio_folder, fn_audio)\n",
    "f_audio, fs_load = librosa.load(path_audio, sr=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute HCQT representation and plot its channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_per_semitone = 3\n",
    "num_octaves = 6\n",
    "n_bins = bins_per_semitone*12*num_octaves\n",
    "num_harmonics = 5\n",
    "num_subharmonics = 1\n",
    "\n",
    "f_hcqt, fs_hcqt, hopsize_cqt = compute_efficient_hcqt(f_audio, fs=22050, fmin=librosa.note_to_hz('C1'), fs_hcqt_target=50, \\\n",
    "                                                    bins_per_octave=bins_per_semitone*12, num_octaves=num_octaves, \\\n",
    "                                                    num_harmonics=num_harmonics, num_subharmonics=num_subharmonics)\n",
    "\n",
    "start_sec = 25\n",
    "show_sec = 50\n",
    "\n",
    "for curr_ax in range(0, 6):\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(10, 3.5))\n",
    "    im = libfmp.b.plot_matrix(np.log(1+1000*np.abs(f_hcqt[:, int(start_sec*fs_hcqt):int(show_sec*fs_hcqt), curr_ax])), Fs=fs_hcqt, ax=ax, cmap='gray_r', ylabel='MIDI pitch')\n",
    "    ax[0].set_yticks(np.arange(1, n_bins+13, 12*bins_per_semitone))\n",
    "    ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+2)])\n",
    "    ax[0].set_xticklabels(np.arange(start_sec-5, show_sec+5, 5))\n",
    "    if curr_ax==0:\n",
    "        ax[0].set_title('subharmonic 1')\n",
    "    elif curr_ax==1:\n",
    "        ax[0].set_title('harmonic 1 (fundamental)')\n",
    "    else:\n",
    "        ax[0].set_title('harmonic ' + str(curr_ax))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "### Optional: Save ###\n",
    "# path_output = ''\n",
    "# np.save(os.path.join(path_output, fn_audio[:-4]+'.npy'), f_hcqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load annotations and convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_folder = os.path.join(basepath, 'data', 'Schubert_Winterreise', 'ann_audio_note')\n",
    "fn_annot = os.path.join(annot_folder, fn_audio[:-4]+'.csv')\n",
    "\n",
    "df = pd.read_csv(fn_annot, sep=';', skiprows=1, header=None)\n",
    "note_events = df.to_numpy()[:, :3]\n",
    "        \n",
    "f_annot_pitch = compute_annotation_array_nooverlap(note_events.copy(), f_hcqt, fs_hcqt, annot_type='pitch', shorten=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot annotation array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(10, 3.5))\n",
    "\n",
    "cfig, cax, cim = libfmp.b.plot_matrix(f_annot_pitch[24:97, int(start_sec*fs_hcqt):int(show_sec*fs_hcqt)], ax=ax, Fs=fs_hcqt, cmap='gray_r', ylabel='MIDI pitch')\n",
    "plt.ylim([0, 73])\n",
    "ax[0].set_yticks(np.arange(0, 73, 12))\n",
    "ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+1)])\n",
    "ax[0].set_title('Multi-pitch annotations (piano roll)')\n",
    "ax[1].set_ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "\n",
    "### Optional: Save ###\n",
    "# path_output_annot = ''\n",
    "# np.save(os.path.join(path_output_annot, song_fn_wav[:-4]+'.npy'), f_annot_pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot sequence of non-repeating annotation vectors (as used for MCTC training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "feat_score = (f_annot_pitch[:, int(start_sec*fs_hcqt):int(show_sec*fs_hcqt)]>0.5).astype(int)\n",
    "\n",
    "char_unique, char_target = np.unique(feat_score, axis=1, return_inverse=True)   # char_unique is the BatchCharacterList\n",
    "char_targ_condensed = np.array([t[0] for t in groupby(char_target)])\n",
    "\n",
    "featvec = np.zeros((12, 0))\n",
    "feat_score_compressed = np.array([char_unique[:, t] for t in char_targ_condensed]).T\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 0.05]}, figsize=(10, 3.5))\n",
    "\n",
    "libfmp.b.plot_matrix(feat_score_compressed[24:97, :], ax=ax, Fs=1, cmap='gray_r', ylabel='MIDI pitch')\n",
    "ax[0].set_xlabel('')\n",
    "ax[0].set_xticks(np.arange(.5, 30.5, 1))\n",
    "ax[0].set_xticklabels('')\n",
    "ax[0].xaxis.grid('on')\n",
    "ax[1].set_ylim([0, 1])\n",
    "plt.ylim([0, 73])\n",
    "ax[0].set_yticks(np.arange(0, 73, 12))\n",
    "ax[0].set_yticklabels([str(24+12*octave) for octave in range(0, num_octaves+1)])\n",
    "ax[0].set_title('Non-repeating targets (for MCTC)')\n",
    "ax[1].set_ylim([0, 1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hopsize_cqt)\n",
    "print(fs)\n",
    "print(fs/hopsize_cqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
